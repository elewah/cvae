{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff87f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "num_epochs = 50\n",
    "latent_dim = 2\n",
    "hidden_dim = 512\n",
    "\n",
    "batch_size = 128\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Lambda(lambda x: x.view(-1) - 0.5),\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform=transform,\n",
    ")\n",
    "# Download and load the test data\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bceb9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "        ...,\n",
       "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
       "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load x from the train_loader\n",
    "x, _ = next(iter(train_loader))\n",
    "# x is a batch of images, shape: [batch_size, 1, 28, 28]\n",
    "# Flatten the images to [batch_size, 784]\n",
    "x = x.view(x.size(0), -1)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36994475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_images(images, labels):\n",
    "    \"\"\"\n",
    "    Display a set of images and their labels using matplotlib.\n",
    "    The first column of `images` should contain the image indices,\n",
    "    and the second column should contain the flattened image pixels\n",
    "    reshaped into 28x28 arrays.\n",
    "    \"\"\"\n",
    "    # Extract the image indices and reshaped pixels\n",
    "    pixels = images.reshape(-1, 28, 28)\n",
    "\n",
    "    # Create a figure with subplots for each image\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=len(images), nrows=1, figsize=(10, 3 * len(images))\n",
    "    )\n",
    "\n",
    "    # Loop over the images and display them with their labels\n",
    "    for i in range(len(images)):\n",
    "        # Display the image and its label\n",
    "        axs[i].imshow(pixels[i], cmap=\"gray\")\n",
    "        axs[i].set_title(\"Label: {}\".format(labels[i]))\n",
    "\n",
    "        # Remove the tick marks and axis labels\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xlabel(\"Index: {}\".format(i))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8a419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class AutoEncoder(nn.Module):\n",
    "#     def __init__(self, input_dim=784, hidden_dim=hidden_dim, latent_dim=latent_dim):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Set the number of hidden units\n",
    "#         self.num_hidden = 4\n",
    "        \n",
    "#         # Define the encoder part of the autoencoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 8, hidden_dim//128), # 2 for mean and variance.\n",
    "#         )\n",
    "        \n",
    "#         # Define the decoder part of the autoencoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim//128, hidden_dim // 8),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 8, hidden_dim // 4),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "#             nn.SiLU(),  # Swish activation function\n",
    "#             nn.Linear(hidden_dim, input_dim),\n",
    "#             nn.Sigmoid(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass the input through the encoder\n",
    "#         encoded = self.encoder(x)\n",
    "#         # Pass the encoded representation through the decoder\n",
    "#         decoded = self.decoder(encoded)\n",
    "#         # Return both the encoded representation and the reconstructed output\n",
    "#         return encoded, decoded\n",
    "import torch.nn as nn\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set the number of hidden units\n",
    "        self.num_hidden = 8\n",
    "        \n",
    "        # Define the encoder part of the autoencoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 256),  # input size: 784, output size: 256\n",
    "            nn.ReLU(),  # apply the ReLU activation function\n",
    "            nn.Linear(256, self.num_hidden),  # input size: 256, output size: num_hidden\n",
    "            nn.ReLU(),  # apply the ReLU activation function\n",
    "        )\n",
    "        \n",
    "        # Define the decoder part of the autoencoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.num_hidden, 256),  # input size: num_hidden, output size: 256\n",
    "            nn.ReLU(),  # apply the ReLU activation function\n",
    "            nn.Linear(256, 784),  # input size: 256, output size: 784\n",
    "            nn.Sigmoid(),  # apply the sigmoid activation function to compress the output to a range of (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the encoder\n",
    "        encoded = self.encoder(x)\n",
    "        # Pass the encoded representation through the decoder\n",
    "        decoded = self.decoder(encoded)\n",
    "        # Return both the encoded representation and the reconstructed output\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d3d892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=784, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Create the autoencoder model and optimizer\n",
    "model = AutoEncoder()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "model.to(device)\n",
    "\n",
    "# # # Create a DataLoader to handle batching of the training data\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     X_train, batch_size=batch_size, shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909b20aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: loss=0.2401\n",
      "Epoch 2/50: loss=0.2313\n",
      "Epoch 3/50: loss=0.2313\n",
      "Epoch 4/50: loss=0.2313\n",
      "Epoch 5/50: loss=0.2313\n",
      "Epoch 6/50: loss=0.2313\n",
      "Epoch 7/50: loss=0.2313\n",
      "Epoch 8/50: loss=0.2313\n",
      "Epoch 9/50: loss=0.2313\n",
      "Epoch 10/50: loss=0.2313\n",
      "Epoch 11/50: loss=0.2313\n",
      "Epoch 12/50: loss=0.2313\n",
      "Epoch 13/50: loss=0.2313\n",
      "Epoch 14/50: loss=0.2313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m     19\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Update the running loss\u001b[39;00m\n\u001b[32m     23\u001b[39m total_loss += loss.item() * data.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CVAE/scripts/vae/lib/python3.12/site-packages/torch/optim/adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data = batch[0]  # Get the images from the batch\n",
    "        # Flatten the images to [batch_size, 784]\n",
    "        # data = data.view(data.size(0), -1)\n",
    "        # Convert to float and move to the device\n",
    "        data = data.to(torch.float32).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        encoded, decoded = model(data)\n",
    "\n",
    "        # Compute the loss and perform backpropagation\n",
    "        loss = criterion(decoded, data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # Print the epoch loss\n",
    "    epoch_loss = total_loss / len(train_loader.dataset)\n",
    "    print(\n",
    "        \"Epoch {}/{}: loss={:.4f}\".format(epoch + 1, num_epochs, epoch_loss)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1594fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to show some images input and output\n",
    "def show_images_input_output(images, labels, outputs):\n",
    "    \"\"\"\n",
    "    Display a set of input images, their labels, and the corresponding output images.\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots for each image\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=2, nrows=len(images), figsize=(10, 3 * len(images))\n",
    "    )\n",
    "\n",
    "    # Loop over the images and display them with their labels\n",
    "    for i in range(len(images)):\n",
    "        # Display the input image\n",
    "        axs[i, 0].imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i, 0].set_title(\"Input: {}\".format(labels[i]))\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Display the output image\n",
    "        axs[i, 1].imshow(outputs[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i, 1].set_title(\"Output: {}\".format(labels[i]))\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "# show_images_input_output(x[5:10], \n",
    "#                          [str(i) for i in range(10)], \n",
    "#                          model(x[:10].to(device))[1].cpu().detach().numpy())\n",
    "# show_images_input_output(x[:10], [str(i) for i in range(10)], model(x[:10])[1].detach().numpy())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4458eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(AutoEncoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add mu and log_var layers for reparameterization\n",
    "        self.mu = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "        self.log_var = nn.Linear(self.num_hidden, self.num_hidden)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Compute the standard deviation from the log variance\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        # Generate random noise using the same shape as std\n",
    "        eps = torch.randn_like(std)\n",
    "        # Return the reparameterized sample\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the encoder\n",
    "        encoded = self.encoder(x)\n",
    "        # Compute the mean and log variance vectors\n",
    "        mu = self.mu(encoded)\n",
    "        log_var = self.log_var(encoded)\n",
    "        # Reparameterize the latent variable\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        # Pass the latent variable through the decoder\n",
    "        decoded = self.decoder(z)\n",
    "        # Return the encoded output, decoded output, mean, and log variance\n",
    "        return encoded, decoded, mu, log_var\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        with torch.no_grad():\n",
    "            # Generate random noise\n",
    "            z = torch.randn(num_samples, self.num_hidden).to(device)\n",
    "            # Pass the noise through the decoder to generate samples\n",
    "            samples = self.decoder(z)\n",
    "        # Return the generated samples\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2ad82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function that combines binary cross-entropy and Kullback-Leibler divergence\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Compute the binary cross-entropy loss between the reconstructed output and the input data\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "    # Compute the Kullback-Leibler divergence between the learned latent variable distribution and a standard Gaussian distribution\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # Combine the two losses by adding them together and return the result\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d60ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(train_loader, learning_rate=1e-3, num_epochs=10, batch_size=32):\n",
    "    # Convert the training data to PyTorch tensors\n",
    "    # X_train = torch.from_numpy(X_train).to(device)\n",
    "    X_train, _ = next(iter(train_loader))\n",
    "\n",
    "    # Create the autoencoder model and optimizer\n",
    "    model = VAE()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a DataLoader to handle batching of the training data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        X_train, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        train_loader.dataset.data\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # data = batch[0]  # Get the images from the batch\n",
    "            # data = data.to(torch.float32).to(device)\n",
    "\n",
    "            # Get a batch of training data and move it to the device\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            encoded, decoded, mu, log_var = model(data)\n",
    "\n",
    "            # Compute the loss and perform backpropagation\n",
    "            KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            loss = criterion(decoded, data) + 3 * KLD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Print the epoch loss\n",
    "        epoch_loss = total_loss / len(train_loader.dataset)\n",
    "        if (epoch + 1) % 100 == 0:  # Print every 10 epochs:\n",
    "            print(\n",
    "                \"Epoch {}/{}: loss={:.4f}\".format(epoch + 1, num_epochs, epoch_loss)\n",
    "            )\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7973a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000: loss=25338.8066\n",
      "Epoch 200/10000: loss=24426.9453\n",
      "Epoch 300/10000: loss=23980.6465\n",
      "Epoch 400/10000: loss=23573.1914\n",
      "Epoch 500/10000: loss=23350.1562\n",
      "Epoch 600/10000: loss=23299.7090\n",
      "Epoch 700/10000: loss=23273.0332\n",
      "Epoch 800/10000: loss=23260.3262\n",
      "Epoch 900/10000: loss=23248.9316\n",
      "Epoch 1000/10000: loss=23256.3047\n",
      "Epoch 1100/10000: loss=23246.4570\n",
      "Epoch 1200/10000: loss=23246.6562\n",
      "Epoch 1300/10000: loss=23244.9746\n",
      "Epoch 1400/10000: loss=23244.6777\n",
      "Epoch 1500/10000: loss=23244.3008\n",
      "Epoch 1600/10000: loss=23244.4785\n",
      "Epoch 1700/10000: loss=23243.9512\n",
      "Epoch 1800/10000: loss=23242.8691\n",
      "Epoch 1900/10000: loss=23243.9062\n",
      "Epoch 2000/10000: loss=23243.2051\n",
      "Epoch 2100/10000: loss=23241.5098\n",
      "Epoch 2200/10000: loss=23240.3906\n",
      "Epoch 2300/10000: loss=23241.0996\n",
      "Epoch 2400/10000: loss=23240.9512\n",
      "Epoch 2500/10000: loss=23240.0254\n",
      "Epoch 2600/10000: loss=23241.4453\n",
      "Epoch 2700/10000: loss=23240.6660\n",
      "Epoch 2800/10000: loss=23239.7578\n",
      "Epoch 2900/10000: loss=23240.5957\n",
      "Epoch 3000/10000: loss=23239.1152\n",
      "Epoch 3100/10000: loss=23241.2285\n",
      "Epoch 3200/10000: loss=23240.8457\n",
      "Epoch 3300/10000: loss=23241.5742\n",
      "Epoch 3400/10000: loss=23239.7656\n",
      "Epoch 3500/10000: loss=23241.5000\n",
      "Epoch 3600/10000: loss=23240.7812\n",
      "Epoch 3700/10000: loss=23240.8457\n",
      "Epoch 3800/10000: loss=23240.3320\n",
      "Epoch 3900/10000: loss=23240.5801\n",
      "Epoch 4000/10000: loss=23240.0176\n",
      "Epoch 4100/10000: loss=23240.9199\n",
      "Epoch 4200/10000: loss=23239.6367\n",
      "Epoch 4300/10000: loss=23240.1914\n",
      "Epoch 4400/10000: loss=23241.0430\n",
      "Epoch 4500/10000: loss=23239.1543\n",
      "Epoch 4600/10000: loss=23239.9316\n",
      "Epoch 4700/10000: loss=23240.9160\n",
      "Epoch 4800/10000: loss=23240.4531\n",
      "Epoch 4900/10000: loss=23240.3730\n",
      "Epoch 5000/10000: loss=23239.9434\n",
      "Epoch 5100/10000: loss=23241.1152\n",
      "Epoch 5200/10000: loss=23239.8867\n",
      "Epoch 5300/10000: loss=23241.0195\n",
      "Epoch 5400/10000: loss=23240.6543\n",
      "Epoch 5500/10000: loss=23240.2305\n",
      "Epoch 5600/10000: loss=23240.0098\n",
      "Epoch 5700/10000: loss=23240.6113\n",
      "Epoch 5800/10000: loss=23241.1387\n",
      "Epoch 5900/10000: loss=23241.1465\n",
      "Epoch 6000/10000: loss=23239.8027\n",
      "Epoch 6100/10000: loss=23239.7910\n",
      "Epoch 6200/10000: loss=23241.0293\n",
      "Epoch 6300/10000: loss=23239.8164\n",
      "Epoch 6400/10000: loss=23240.5625\n",
      "Epoch 6500/10000: loss=23240.7012\n",
      "Epoch 6600/10000: loss=23240.1621\n",
      "Epoch 6700/10000: loss=23240.9160\n",
      "Epoch 6800/10000: loss=23239.8008\n",
      "Epoch 6900/10000: loss=23240.5176\n",
      "Epoch 7000/10000: loss=23241.0820\n",
      "Epoch 7100/10000: loss=23239.5195\n",
      "Epoch 7200/10000: loss=23240.0723\n",
      "Epoch 7300/10000: loss=23240.4883\n",
      "Epoch 7400/10000: loss=23239.6055\n",
      "Epoch 7500/10000: loss=23239.2969\n",
      "Epoch 7600/10000: loss=23241.2422\n",
      "Epoch 7700/10000: loss=23240.6543\n",
      "Epoch 7800/10000: loss=23239.7988\n",
      "Epoch 7900/10000: loss=23239.9297\n",
      "Epoch 8000/10000: loss=23240.0488\n",
      "Epoch 8100/10000: loss=23239.9238\n",
      "Epoch 8200/10000: loss=23240.2266\n",
      "Epoch 8300/10000: loss=23240.6953\n",
      "Epoch 8400/10000: loss=23241.5215\n",
      "Epoch 8500/10000: loss=23240.1309\n",
      "Epoch 8600/10000: loss=23239.5391\n",
      "Epoch 8700/10000: loss=23240.8320\n",
      "Epoch 8800/10000: loss=23239.9707\n",
      "Epoch 8900/10000: loss=23240.4688\n",
      "Epoch 9000/10000: loss=23241.5293\n",
      "Epoch 9100/10000: loss=23240.0547\n",
      "Epoch 9200/10000: loss=23239.3574\n",
      "Epoch 9300/10000: loss=23239.6562\n",
      "Epoch 9400/10000: loss=23241.5195\n",
      "Epoch 9500/10000: loss=23241.3574\n",
      "Epoch 9600/10000: loss=23240.7891\n",
      "Epoch 9700/10000: loss=23240.0098\n",
      "Epoch 9800/10000: loss=23240.7363\n",
      "Epoch 9900/10000: loss=23240.1973\n",
      "Epoch 10000/10000: loss=23239.1074\n"
     ]
    }
   ],
   "source": [
    "# train the VAE model\n",
    "vae_model = train_vae(train_loader, learning_rate=1e-3, num_epochs=10000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da8a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(train_loader))\n",
    "# len(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a866d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try to show some images input and output\n",
    "def show_images_input_output(images, labels, outputs):\n",
    "    \"\"\"\n",
    "    Display a set of input images, their labels, and the corresponding output images.\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots for each image\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=2, nrows=len(images), figsize=(10, 3 * len(images))\n",
    "    )\n",
    "\n",
    "    # Loop over the images and display them with their labels\n",
    "    for i in range(len(images)):\n",
    "        # Display the input image\n",
    "        axs[i, 0].imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i, 0].set_title(\"Input: {}\".format(labels[i]))\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Display the output image\n",
    "        axs[i, 1].imshow(outputs[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i, 1].set_title(\"Output: {}\".format(labels[i]))\n",
    "        axs[i, 1].axis('off')\n",
    "\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "show_images_input_output(x[5:10], \n",
    "                         [str(i) for i in range(10)], \n",
    "                         vae_model.forward(x[:10].to(device))[1].cpu().detach().numpy())\n",
    "# show_images_input_output(x[:10], [str(i) for i in range(10)], model(x[:10])[1].detach().numpy())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14edcd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# print train_x for train_loader\n",
    "print(train_loader.dataset.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
